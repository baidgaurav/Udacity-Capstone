{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nprint(os.listdir(\"../input\"))\n\ntraining_data= pd.read_csv(\"../input/train.csv\")\ntesting_data= pd.read_csv(\"../input/test.csv\")\n\nprint(training_data.shape)\nprint(testing_data.shape)\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"a7bd2216d3c674ac70873be5aaa799b1b0942dab","collapsed":true},"cell_type":"code","source":"print(\"First few records from the training data set\")\ntraining_data.head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"32cb743d586bd94f6ced4d613b96f808b704f0c9","collapsed":true},"cell_type":"code","source":"print(\"First few records from the testing data set\")\ntesting_data.head(15)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"953aed74b3692fda47b40544d7801acf6308a4e1"},"cell_type":"markdown","source":"Exploratory Data Analyis  to look for Duplicate number of records in the training data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"##Exploratory Data Analyis \nDup_rec= training_data['is_duplicate'].value_counts()\nDup_rec_per =(Dup_rec/Dup_rec.sum())\n# Looking at duplicate records in the training data\nprint (\"Number of records for duplicate and not duplicate :\\n %s\" %Dup_rec)\nprint (\"Percentage of duplicated records in training data :\\n %s\" %Dup_rec_per)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19ab1e8d9f29effcb96c58fea447bcb79ec0c7fe"},"cell_type":"markdown","source":"Identifying number of unique questions in the training data set "},{"metadata":{"trusted":true,"_uuid":"cd65a9eba18c16d70a9f151c4aa0b2ec2c8cdf9f","collapsed":true},"cell_type":"code","source":"total_questions = pd.Series(training_data['qid1'].tolist() + training_data['qid2'].tolist())\nprint('Total unique questions in column qid1 and qid2 : {}'.format(len(np.unique(total_questions))))\nprint(\"Number of questions with more than one occurence in the training data: {}\".format(np.sum(total_questions.value_counts()> 1)))\n\n#creating a histogram for question occurences\n\nplt.figure(figsize=(10,5))\nplt.hist(total_questions.value_counts(), bins=30)\nplt.yscale('log', nonposy= 'clip')\nplt.title('Question occurence counts')\nplt.xlabel('Number of Occurences for Question')\nplt.ylabel('total number of questions')\nprint()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7737bcda3f4d43f21eb472e67dd4ccd2d283c32e"},"cell_type":"markdown","source":"* 111,780 questions appeared multiple times in training data out of 537933 unique questions \n* As seen in the below histogram, majority of the questions appeared more than once\n* There is a vast majority of questions that appeared less than 60 times\n* A very small portion of the questions appeared more than 100 times\n"},{"metadata":{"trusted":true,"_uuid":"e5e4deb068801e4214114458a3db52492f94f80b","collapsed":true},"cell_type":"code","source":"#Checking for Null Values in the data set (Both training and testing)\nprint(\"Number of null values in training data set: %d\" %training_data.isnull().sum().sum())\nprint(\"Number of null values in testing data set: %d\" %testing_data.isnull().sum().sum())\n\n#Find the row id for Null values in the training data set\nprint(training_data[pd.isnull(training_data).any(axis=1)])\n\n#Find the row id for Null values in the testing data set\nprint(testing_data[pd.isnull(testing_data).any(axis=1)])\n\n#Replacing the null vlaues with a string 'NA'\ntraining_data.ix[training_data['question1'].isnull(),['question1']] = 'NA'\ntraining_data.ix[training_data['question2'].isnull(),['question2']] = 'NA'\ntesting_data.ix[testing_data['question1'].isnull(),['question1']] = 'NA'\ntesting_data.ix[testing_data['question2'].isnull(),['question2']] = 'NA'\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aeaf669de3a48ec7add1ad222c7f9ac8db893d53"},"cell_type":"markdown","source":"* training data has 3 null values and testing data has 6 null values."},{"metadata":{"_uuid":"43106f3eacb743e6c5a8b4b5a83d4df17bb024e5"},"cell_type":"markdown","source":"**Data Pre-Prcoessing **"},{"metadata":{"trusted":true,"_uuid":"56e1e394a81a950374c71f6c0e339fa310f7b752","collapsed":true},"cell_type":"code","source":"##Fucntions for different preprocessing steps \n## https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\nimport string\nfrom string import punctuation\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nstemmer=PorterStemmer()\nstop_words = set(stopwords.words('english'))\n# Make the questions lower case\ndef lower_case(x):\n    return x.lower()\n\n#removing stop words\ndef remove_stop_words(x):\n    x = x.split()\n    x = [s for s in x if s not in stop_words]\n    x = \" \".join(x)\n    return x\n\n##removing punctuations \ndef remove_punctuation(x):\n    punc_string = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n    for i in x.lower():\n        if i in punc_string:\n            x = x.replace(i, \"\")\n    return x        \n##Using Stemmer for converting the words to their roots\ndef stem_port(x):\n    x = \" \".join([stemmer.stem(i) for i in x.lower().split(\" \")])\n    return x\n    \nprint(\"Process Finished\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7311f8fc5b90b1a414db28d9b1b2a54f6ad5dbd4"},"cell_type":"markdown","source":"Running a simple test on a sentence to see if the fucntions are working properly "},{"metadata":{"trusted":true,"_uuid":"6eb958b1b7483ac64f6a29581aad17db94f8330e","collapsed":true},"cell_type":"code","source":"##testing for removing lowe case, stop words, removing punctuation, stem words\nlower_cs = lower_case(\"Hi, i HOPE you are doing well and feeling amazing!!!!!!!!!\")\nprint(lower_cs)\nclean_st = remove_stop_words(lower_cs)\nprint(clean_st)\nclean_pc = remove_punctuation(clean_st)\nprint(clean_pc)\nstem_words = stem_port(clean_pc)\nprint(stem_words)\n#dir(string)\n\nprint(\"Process finished\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77f22952f61bf6087deccd1bfe14ae112c3714ad"},"cell_type":"markdown","source":"Treating Null values and unnecesary columns "},{"metadata":{"trusted":true,"_uuid":"7e8d1b9194aa6da42cc087942b8d13ea1f3b6840","collapsed":true},"cell_type":"code","source":"#Dropping id,qid1 and qid2 as it does not provide valuable imputs\ntraining_data.drop(['id', 'qid1', 'qid2'], axis = 1, inplace = True)\n##training_data.fillna(value = \"\", axis = 0, inplace = True)\n\n#split into features and results\nresult_data = training_data['is_duplicate']\nraw_training_data = training_data\n\n#check training features and results are as expected\ndisplay(result_data.head(10))\n#display(raw_training_data.head(10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0bef371df33e9803ef002c6cd21b422467546554"},"cell_type":"markdown","source":"Applying preprocessing functions to Training and Testing data set"},{"metadata":{"trusted":true,"_uuid":"b059d8a5981ba5bb01b0a4899b2e33ea5e993495","collapsed":true},"cell_type":"code","source":"#### Preprocessing Training data\n#make all the questions lower case\nraw_training_data[\"question1\"] = raw_training_data[\"question1\"].apply(lower_case)\nraw_training_data[\"question2\"] = raw_training_data[\"question2\"].apply(lower_case)\n\n#remove stop words from the question \nraw_training_data[\"question1\"] = raw_training_data[\"question1\"].apply(remove_stop_words)\nraw_training_data[\"question2\"] = raw_training_data[\"question2\"].apply(remove_stop_words)\n\n#remove punctuation from the question \nraw_training_data[\"question1\"] = raw_training_data[\"question1\"].apply(remove_punctuation)\nraw_training_data[\"question2\"] = raw_training_data[\"question2\"].apply(remove_punctuation)\n\n#Stemming for all the words in the question\nraw_training_data[\"question1\"] = raw_training_data[\"question1\"].apply(stem_port)\nraw_training_data[\"question2\"] = raw_training_data[\"question2\"].apply(stem_port)\nprint(\"process finished\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f53d02a24a55c9c2af55c2aaac0b6695d8dd2174","collapsed":true},"cell_type":"code","source":"#### Preprocessing Testing data\nraw_testing_data = testing_data\n#make all the questions lower case\nraw_testing_data[\"question1\"] = raw_testing_data[\"question1\"].apply(lower_case)\nraw_testing_data[\"question2\"] = raw_testing_data[\"question2\"].apply(lower_case)\n\n#remove stop words from the question \nraw_testing_data[\"question1\"] = raw_testing_data[\"question1\"].apply(remove_stop_words)\nraw_testing_data[\"question2\"] = raw_testing_data[\"question2\"].apply(remove_stop_words)\n\n#remove punctuation from the question \nraw_testing_data[\"question1\"] = raw_testing_data[\"question1\"].apply(remove_punctuation)\nraw_testing_data[\"question2\"] = raw_testing_data[\"question2\"].apply(remove_punctuation)\n\n#Stemming for all the words in the question\nraw_testing_data[\"question1\"] = raw_testing_data[\"question1\"].apply(stem_port)\nraw_testing_data[\"question2\"] = raw_testing_data[\"question2\"].apply(stem_port)\nprint(\"process finished\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6245454c887b72bfb41e2e3a37d35f66ef575baf","collapsed":true},"cell_type":"code","source":"#saving the preprocessed files for repeated use \nraw_train_save = raw_training_data\nraw_test_save = raw_testing_data\n\n#Checking to see if the preprocessing steps were implemented properly\ndisplay(raw_training_data.head(20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed5f11ef848136c2c6bb5303b8db76acc07ce8e6","collapsed":true},"cell_type":"code","source":"display(raw_testing_data.head(20))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c23bb91478e1e309838bb7e075d171430ab84e6f"},"cell_type":"markdown","source":"Feature Engineering ( Creating new feature using exsiting data)"},{"metadata":{"trusted":true,"_uuid":"63c9afbd239fa79e560fdcef9788965d4b637fa6","collapsed":true},"cell_type":"code","source":"##creating new features in the data set\n#Finding number of common words in question pairs\ndef common_words(w):\n    common = 0 \n    que1 = w[\"question1\"].lower().split()\n    que2 = w[\"question2\"].lower().split() \n    \n    for i in que1:\n        u = i\n        if i in que2:\n            common+= 1            \n    return common\n\n#function to find the difference in number of words in questions \ndef diff_words(w):\n    try:\n        que1len = len(w[\"question1\"].split())\n        que2len = len(w[\"question2\"].split())\n        return np.absolute(que1len - que2len) \n    except:\n        return 10\n#finding ratio of common words between question pairs \ndef ratio_common_words(w):\n    try: \n        ratio =  (1.0 * common_words(w) / (len(w[\"question1\"].split()) + len(w[\"question2\"].split())))\n        return ratio\n    except:\n        return 0 \n    \nprint(\"Process Finished\")    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b825ecbc501c44dbc937d318b106548e659f3309","collapsed":true},"cell_type":"code","source":"## Distribution of Common Words (Duplicate and Not Duplicate)\ncmmn_words = raw_training_data.apply(common_words, axis=1)\nplt.figure(figsize=(15, 5))\nplt.hist(cmmn_words[raw_training_data['is_duplicate'] == 0],bins=20, histtype=\"stepfilled\", alpha = 0.75, label=\"Not duplicate\")\nplt.hist(cmmn_words[raw_training_data['is_duplicate'] == 1],bins=20, histtype=\"stepfilled\", alpha = 0.75, label=\"Duplicate\")\nplt.yscale('log', nonposy='clip')\nplt.legend()\nplt.title('distribution over common words', fontsize=15)\nplt.xlabel('Common words', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8cddbec195d44e0c11734c3c3b992287be85279f","collapsed":true},"cell_type":"code","source":"## Distribution of difference of Words (Duplicate and Not Duplicate)\ndiff_words_n = raw_training_data.apply(diff_words, axis=1)\nplt.figure(figsize=(15, 5))\nplt.hist(diff_words_n[raw_training_data['is_duplicate'] == 0], bins=20, histtype=\"stepfilled\", alpha = 0.75, label=\"Not duplicate\")\nplt.hist(diff_words_n[raw_training_data['is_duplicate'] == 1], histtype=\"stepfilled\", alpha = 0.75, label=\"Duplicate\")\nplt.yscale('log', nonposy='clip')\nplt.legend()\nplt.title('distribution over diff of words', fontsize=15)\nplt.xlabel('diff of words', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dec31975fd6607842dc37710ae4914589c30e44c","collapsed":true},"cell_type":"code","source":"## Distribution of ratio Common Words (Duplicate and Not Duplicate)\nra_cmmn_words = raw_training_data.apply(ratio_common_words, axis=1)\nplt.figure(figsize=(15, 5))\nplt.hist(ra_cmmn_words[raw_training_data['is_duplicate'] == 0], bins=20, histtype=\"stepfilled\", alpha = 0.75, label=\"Not duplicate\")\nplt.hist(ra_cmmn_words[raw_training_data['is_duplicate'] == 1], histtype=\"stepfilled\", alpha = 0.75, label=\"Duplicate\")\nplt.yscale('log', nonposy='clip')\nplt.legend()\nplt.title('distribution over ratio of common words', fontsize=15)\nplt.xlabel('ratio of Common words', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b78ff2e77dcb538716bec026a5ffeb0075a251cc"},"cell_type":"markdown","source":"Implementing tf-idf / cosine similarity "},{"metadata":{"trusted":true,"_uuid":"e8c07d3693f804cc41c981b41b559f07f502be2b","collapsed":true},"cell_type":"code","source":"#cosine Similarity/TFIDF vectorizer \n#putting all the questions (question1 and question2) in a single list for TFIDF\n\nTraining_co_set = pd.Series(raw_training_data['question1'].tolist() + raw_training_data['question2'].tolist()).astype(str)\nTesting_co_set = pd.Series(raw_testing_data['question1'].tolist() + raw_testing_data['question2'].tolist()).astype(str)\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(min_df = 2)\n\ntfidf_new= tfidf_vectorizer.fit(pd.concat((raw_training_data.ix[:,'question1'],raw_training_data.ix[:,'question2'])).unique())\n\n#Generating new vectors for individual question\ntrain_Que1_tfidf = tfidf_vectorizer.transform(raw_training_data.ix[:,'question1'])\ntrain_Que2_tfidf = tfidf_vectorizer.transform(raw_training_data.ix[:,'question2'])\n\nprint(\"Question 1 Shape: \", train_Que1_tfidf.shape)\nprint(\"Question 2 Shape: \", train_Que2_tfidf.shape)\n\n#applying cosine similarites to the vectors for training data \nfrom sklearn.metrics.pairwise import cosine_similarity\ncosine_train_new = []\nfor i in range(0,404290):\n    cosine_train_new.append((cosine_similarity(train_Que1_tfidf[i],train_Que2_tfidf[i]))[0][0])\n    i = i +1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7e49a9f12a72e2ed28b4076cf52b6b497c91cd1","collapsed":true},"cell_type":"code","source":"raw_training_data['cosine_trfs'] = cosine_train_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"853098afef1c570d366f473efa958458facbb00e","collapsed":true},"cell_type":"code","source":"#label distribution over Cosine Similarity\nplt.figure(figsize=(15, 5))\nplt.hist(raw_training_data[raw_training_data['is_duplicate']== 0]['cosine_trfs'], bins=50, range=[0, 1], normed=True, alpha=0.5, label='not duplicate')\nplt.hist(raw_training_data[raw_training_data['is_duplicate']== 1]['cosine_trfs'], bins=50, range=[0, 1], normed=True, alpha=0.5, label='duplicate')\nplt.title('Distribution', fontsize=20)\nplt.legend()\nplt.xlabel('Cosine Similarity', fontsize=20)\nplt.ylabel('Distribution', fontsize=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7218ca2e665fb5e9d2be2abac5fb898d84cfe32","collapsed":true},"cell_type":"code","source":"##Appying Cosine similarity to Testing data\ntest_Que1_tfidf = tfidf_vectorizer.transform(raw_testing_data.ix[:,'question1'])\ntest_Que2_tfidf = tfidf_vectorizer.transform(raw_testing_data.ix[:,'question2'])\nprint(\"Question 1 Shape: \", test_Que1_tfidf.shape)\nprint(\"Question 2 Shape: \", test_Que2_tfidf.shape)\n\ncosine_test_new = []\nfor i in range(0,2345796):\n    cosine_test_new.append((cosine_similarity(test_Que1_tfidf[i],test_Que2_tfidf[i]))[0][0])\n    i = i + 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3f5e80414a87bc5af46a2a4962eb6282aeb5046"},"cell_type":"markdown","source":"Feature selection for data"},{"metadata":{"trusted":true,"_uuid":"953deed7dcec609ac70e71b6085a942c0aef09c5","collapsed":true},"cell_type":"code","source":"#adding more features to the original data set\nFeature_train_Data = pd.DataFrame(dtype='float64')\nFeature_test_Data = pd.DataFrame(dtype='float64')\n\nFeature_train_Data['que1len'] = raw_training_data['question1'].str.len()\nFeature_train_Data['que2len'] = raw_training_data['question2'].str.len()\nFeature_train_Data['que1word'] = raw_training_data['question1'].apply(lambda x: len(str(x).split(\" \")))\nFeature_train_Data['que2word'] = raw_training_data['question2'].apply(lambda y: len(str(y).split(\" \")))\n#Feature_train_Data['Common_words'] = raw_training_data.apply(common_words, axis=1)\nFeature_train_Data['diff_words'] = raw_training_data.apply(diff_words, axis=1)\nFeature_train_Data['ratio_common_words'] = raw_training_data.apply(ratio_common_words, axis=1)\nFeature_train_Data['cosine_trfs'] = cosine_train_new\n\n#Feature_train_Data.head()\n\nFeature_test_Data['que1len'] = raw_testing_data['question1'].str.len()\nFeature_test_Data['que2len'] = raw_testing_data['question2'].str.len()\nFeature_test_Data['que1word'] = raw_testing_data['question1'].apply(lambda x: len(str(x).split(\" \")))\nFeature_test_Data['que2word'] = raw_testing_data['question2'].apply(lambda y: len(str(y).split(\" \")))\n#Feature_test_Data['Common_words'] = raw_testing_data.apply(common_words, axis=1)\nFeature_test_Data['diff_words'] = raw_testing_data.apply(diff_words, axis=1)\nFeature_test_Data['ratio_common_words'] = raw_testing_data.apply(ratio_common_words, axis=1)\nFeature_test_Data['cosine_trfs'] = cosine_test_new\n\nFeature_ytrain_Data = raw_training_data['is_duplicate'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32c300b039ae54119ce41962fc85b61c466ef70d","collapsed":true},"cell_type":"code","source":"Feature_train_Data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fec8b852bfeb11a4f09919b105512ce9dc46e69e","collapsed":true},"cell_type":"code","source":"Feature_test_Data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12f0f665ebc4543f2c0d36f0418ef66ff83267e7"},"cell_type":"markdown","source":"**Scaling the data** "},{"metadata":{"trusted":true,"_uuid":"c4cfcc22124a61255ba61e166f07ec7a2de169a8","scrolled":true,"collapsed":true},"cell_type":"code","source":"#Scaling all the features: \nfrom sklearn import preprocessing\n\nmin_max_scaler = preprocessing.MinMaxScaler()\n#feature_list = [\"Common_words\",\"diff_words\",\"ratio_common_words\", \"cosine_trfs\"]\nfeature_list = [\"que1len\",\"que2len\",\"que1word\",\"que2word\",\"diff_words\",\"ratio_common_words\",\"cosine_trfs\"]\nscaled = min_max_scaler.fit_transform(Feature_train_Data[feature_list])\n\nscaled_features = pd.DataFrame(data = scaled, columns=feature_list)\ndisplay(scaled_features.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5cdd21c4ab46b52a586110075d8fe6dff9e2232"},"cell_type":"markdown","source":"Do not use below section"},{"metadata":{"trusted":true,"_uuid":"9e11fcbf04736d32c44da7ee16de80f805285b67","collapsed":true},"cell_type":"code","source":"##Rebalancing the data : Section taken from https://www.kaggle.com/anokas/data-analysis-xgboost-starter-0-35460-lb \n##According to the analysis in above blog, The training data has 37% of the positive class while testing data has only 17%\n#of positive class. rebalacing the data will make sure the training data also will have 17% of positive class in the data. \n#Adding rebalancing data section helped improve the log loss score on XGboost significantly \n\n#pos_train = Feature_train_Data[Feature_ytrain_Data == 1]\n#neg_train = Feature_train_Data[Feature_ytrain_Data == 0]\n\n# Now we oversample the negative class\n# There is likely a much more elegant way to do this...\n#p = 0.165\n#scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n#while scale > 1:\n #   neg_train = pd.concat([neg_train, neg_train])\n #   scale -=1\n#neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n#print(len(pos_train) / (len(pos_train) + len(neg_train)))\n\n#x_train = pd.concat([pos_train, neg_train])\n#y_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n#del pos_train, neg_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"dc74e693c7b371ba5eaa2267e45127e901a60811","collapsed":true},"cell_type":"code","source":"# creating a benchmark model for the data set\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import log_loss\n\nx_train, x_valid, y_train, y_valid = train_test_split(scaled_features, Feature_ytrain_Data, test_size=0.2, random_state=0)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d62b01f5509e1d4d9234c92a95dda3b705673c8a"},"cell_type":"markdown","source":"**Random Forest Implementation**"},{"metadata":{"trusted":true,"_uuid":"b3c35e3bc5417fcd6a1b2008d47adb2ece37dfd8","collapsed":true},"cell_type":"code","source":"##Creating a list of parameters to be used in GridSearch (n_estimators : 10,20,50,100)\nparam = {'n_estimators':[300], 'random_state':[0]}\nrandom_clf = RandomForestClassifier() \n\nrn_grid_clf = GridSearchCV(random_clf, param)\nrn_grid_clf = rn_grid_clf.fit(x_train, y_train)\n\nprint(rn_grid_clf.best_estimator_)\n\n#random forest predictor\nrandom_pred = rn_grid_clf.predict_proba(x_valid)\nrandom_pred = random_pred[:,1]\n\n#finding the log loss \nprint(\"The log loss error is : \")\nlog_loss(y_valid, random_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c81459782268e3ce47ed26866dbe620e565f1de9","collapsed":true},"cell_type":"code","source":"#Creating predictions for the test dataset\nrand_x_test = rn_grid_clf.predict_proba(Feature_test_Data)\nrand_x_test = rand_x_test[:,1]\n\n\nRFC_data_csv = pd.DataFrame()\nRFC_data_csv ['test_id'] = testing_data['test_id']\nRFC_data_csv['is_duplicate'] = rand_x_test\nRFC_data_csv.to_csv('RFC_export.csv', index = False)\nprint(\"Export Successful\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2aaccebb71ce4f1d6ceb8209230aeb4dd672901b"},"cell_type":"markdown","source":"**Logistic Regression Implementation**"},{"metadata":{"trusted":true,"_uuid":"10f28d030a4af3e8721ff8986a5addc1e07c485d","collapsed":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n#using sag solver as it is good for large data sets and faster as compared to Liblinear\nparam_reg = {'solver':['sag'], 'C':[.0011], 'random_state':[0]}\nlog_clf = LogisticRegression()\n\nlr_grid_clf = GridSearchCV(log_clf, param_reg)\n\nlr_grid_clf = lr_grid_clf.fit(x_train, y_train)\nprint(\"The best estimator is : \", lr_grid_clf.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24a7d8398d15e9f7d177c08e4eb313bc7b320f3f","collapsed":true},"cell_type":"code","source":"log_random_pred = lr_grid_clf.predict_proba(x_valid)\nlog_random_pred = log_random_pred[:,1]\n\n#finding the log loss on the validation data set\nprint(\"The log loss error is : \")\nlog_loss(y_valid, log_random_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc466a5921b5cfc0bce741ff81df51965032fc61","collapsed":true},"cell_type":"code","source":"#Creating predictions for the test dataset\nlog_x_test = lr_grid_clf.predict_proba(Feature_test_Data)\nlog_x_test = log_x_test[:,1]\n\nlog_data_csv = pd.DataFrame()\nlog_data_csv ['test_id'] = testing_data['test_id']\nlog_data_csv['is_duplicate'] = log_x_test\nlog_data_csv.to_csv('log_export.csv', index = False)\nprint(\"Export Successful\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48ce8075887e59baaf7744ae899dd375600b2dd1"},"cell_type":"markdown","source":"**Implementing XGBoost Classifier**"},{"metadata":{"trusted":true,"_uuid":"03be237a838383228c2c83deb734ae89be8e954f","collapsed":true},"cell_type":"code","source":"import xgboost as xgb\n# parameters for xgboost\nparams = {}\nparams['max_depth'] = 8\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.4\nparams['objective'] = 'binary:logistic'\n#params['gamma'] = 2\n#params['subsample'] = 0.5\n#params['colsample_bytree'] = 0.8\n#params['scale_pos_weight'] = 10\n\nprint(params)\nxgb_train = xgb.DMatrix(x_train, label=y_train)\nxgb_valid = xgb.DMatrix(x_valid, label=y_valid)\n\nwatchlist = [(xgb_train, 'training'), (xgb_valid, 'validation')]\n\nbst = xgb.train(params, xgb_train, 500, watchlist, early_stopping_rounds= 50, verbose_eval=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60cec5798a3a123d5dfc5061a6a12a9bdd784041","collapsed":true},"cell_type":"code","source":"xgb_test = xgb.DMatrix(Feature_test_Data)\npredicted_test_op = bst.predict(xgb_test)\n\n##output the data to csv for test\nxgb_data_csv = pd.DataFrame()\nxgb_data_csv ['test_id'] = testing_data['test_id']\nxgb_data_csv['is_duplicate'] = predicted_test_op\nxgb_data_csv.to_csv('xgb_export.csv', index = False)\nprint(\"Export Successful\")\n\nxgb_data_csv.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd9c3fffccb24d6df27c9c34086761c1e81c9aa8"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}